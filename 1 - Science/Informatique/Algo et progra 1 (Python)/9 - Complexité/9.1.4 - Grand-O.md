---
tags:
  - Note_done
  - Informatique
source: UMons - Algorithme et programmation 1
---


Link :
_Calculus : Dérivée de fonctions d'une variable_
1. [[1.5.13 - Déf = Petit-o]]

_Math discrète : Relation binaire_
1. [[1.2.5 - Relation transitive]]

_Math élémentaire : Logique_
1. [[0.2.8 - Preuve par l'absurde]]

_Python : Complexité_
1. [[9.1.1 - Efficacité des algorithmes]]
2. [[9.1.3 - Fonction T(n), le temps d'exécution]]
3. [[9.1.5 - Complexité]]

# Définition
Soit $f : \mathbb{N} \to \mathbb{R}$ 
La notation grand-O permet de décrire le taux de croissance d'une fonction et est définie par $$f(n)\in O(g(n))\Longleftrightarrow\exists n_0\in\mathbb{N},c\in\mathbb{R}^+\mathrm{~t.q.~}\forall n\geq n_0:f(n)\leq c\mathrm{~}g(n)$$
On dit alors que "$f$ est en grand-O de $g$" si $f$ ne croît pas plus vite que $g$

**Illustration** :
![[Pasted image 20231107105521.png]]
1. $n_0 \ge 0$ permet d’éviter les effets de bord et montre que ce n’est pas parce qu’un algorithme est plus rapide sur des petites valeurs, qu’il le sera encore asymptotiquement càd une courbe qui s'approche indéfiniment à des valeurs infinies
2. $c > 0$ exprime le fait que les constantes ne sont pas (très) importantes

_Remarque_ :
1. Les fonctions de complexité sont de fonctions $\mathbb{N} \longrightarrow \mathbb{R}$
2. Ne pas confondre avec le petit-$o$ 
	1. Le petit-$o$ : On s’intéresse à trouver des fonctions qui rendent plus rapidement vers 0
	2. Le grand-$O$ : On s’intéresse aux caractères asymptomatiques de la fonction 

#### Exemple
##### Exemple 1
Soit $f(n) = 1000n$ et $g(n)= n$, alors $f(n) \in O(g(n))$ 
En effet, si $n_0 =0$ et $c=1000$, alors $f(n)= 1000n \le 1000n = 1000g(n),\ \forall n \ge 0$
##### Exemple 2
Soit $f(n)=2n^2$ et $g(n)=n^2$, alors $f(n) \in O(g(n))$ 
En effet, si $n_0 =0$ et $c=2$, alors $f(n) \le 2g(n),\ \forall n \ge0$ 

##### Exemple 3 
Soit $f(n)= (n+1)^2$ et $g(n) = n^2$, alors $f(n) \in O(g(n))$ 
En effet, si $n_0 = 1$ et $c=4$, alors $f(n)= (n+1)^2 = n^2+2n+1 \le n^2 +2n^2 + n^2 = 4g(n),\ \forall n \ge 1$
_Remarque_ :
1. L’inégalité n’est pas vrai si $n =0$, d’où $n_0=1$ 

##### Exemple 4
Prouvez que $n^3 \notin O(n^2)$. 
Par l'absurde, supposons que $n^3 \in O(n^2)$, alors il existe deux constantes $n_0$ et $c$ telles que $n^3 \le c\ n^2,\ \forall n \ge n_0$.
Cela signifie que $$c \ge \frac{n^3}{n^2}= n, \forall n \ge max(n_0, 1)$$ ce qui est une contradiction car $c$ est une constante

_Remarque_ :
1. $\max(n_0,1)$ est là pour éviter une division par 0 si $n_0 = 0$

### A quoi sert la notation grand-O ?
On va décrire le taux de croissance de la fonction $T(n)$ en utilisant une notation spécifique, la notation O (prononcer “grand-O”). 
On parle aussi de la complexité (sous-entendu “dans le pire des cas”) de la fonction. 
Ceci permettra de classer les fonctions et donc les temps d’exécution des algorithmes – selon leur type de croissance, indépendamment des constantes et autres détails techniques.
## Propriétés : Simplification des expressions grand-O
La notation grand-O permet de simplifier les fonctions de complexité.

#### A) Lemme : Les facteurs constantes ne sont pas importants 
Pour toute valeur constante $d >0$ et toute fonction de complexité $f(n)$ 
On a $$f(n)\in O(d\cdot f(n))\mathrm{~et~}d\cdot f(n)\in O(f(n))$$
##### Preuve de (1)
1. _Cas 1_ :
Soit $c = \frac{1}{d}$ et $n_0 = 0$,
on a $$\forall n \ge 0,\ f(n) = (c.d)f(n) = c(d.f(n))$$
et donc $f(n) \in O(d.f(n))$

2. _Cas 2_ :
Pour le 2e cas, prenons $c = d$. 
On a $$\forall n \ge 0,\ f(n) = (c.\frac{1}{d})f(n) = c(\frac{1}{d}.f(n))$$ et donc $f(n) \in O(\frac{1}{d}.f(n))$

###### Exemple
1. $1000n^2$ et $\frac{n^2}{1000} \in O(n^2)$

#### 2) Lemme : Les termes d'ordres inférieurs sont négligeables 
Soit une fonction de complexité $f(n)$ polynomiale et dont le coefficient du terme de plus grand degré est positif i.e. $$f(n):=a_kn^k+a_{k-1}n^{k-1}+\ldots+a_2n^2+a_1n+a_0$$ où $a_k > 0$ 
Alors $$f(n)\in O\left(n^k\right)$$
##### Preuve de (2)
Soit $n_0 = 1$ et $c = \sum_{i=0}^{k} max(a_i,\ 0)$ (somme des coefficients positifs)
Donc $c$ est bien une constante strictement positive car $a_k > 0$
Alors $$\forall n\ge 1,\ f(n) \le cn^k$$
En effet, pour chaque coefficient $a_j$ :
1. Si $a_j \le 0$, on a certainement $\forall n \ge 1,\ a_jn^j \le 0$
2. Si $a_j > 0$, on a certainement $\forall n \ge 1,\ a_jn^j \le a_kn^k$ car $j \le k$

###### Exemple
1. Soit $f(n)\ := 3n^5 + 10n^4 - 100n^3 + n + 1$. Alors $f(n) \in O(n^5)$. En effet, 
	1. si $c = 3 + 10 - 0 + 1 + 1 =15$, 
	2. alors $f(n) \le 15n^5,\ \forall n \ge 1$

#### 3) Lemme : La somme d'une fonction $g(n)$, qui croît moins vite avec une autre, qui elle croît vite, est négligeable
1. Si $g(n)$ croît moins vite que $h(n)$, 

alors $g(n) + h(n) \in O(h(n))$ 
Donc on peut négliger ce qui croît moins vite.

###### Exemple
1. On sait que toute exponentielle croît plus vite que tout polynôme : $$2^n + n^3 \in O(2^n)$$ 
#### 4) Lemme : Transitivité des expressions grand-O
Grand-O est une relation transitive, i.e. que 
1. si $f \in O(g)$ et $g \in O(h)$, 

alors $f \in O(h)$

##### Preuve de (4)
Par définition, $$\begin{aligned}&f(n)\leq c_1g(n),\quad\forall n\geq n_1\\&g(n)\leq c_2h(n),\quad\forall n\geq n_2\end{aligned}$$
Soit $n_0 = max(n_1, n_2)$ et $c_0 = c_1 . c_2$, alors, $$f(n)\leq c_1g(n)\leq c_1c_2h(n)=c_0h(n),\quad\forall n\geq n_0$$

## Règles pour exprimer la complexité d'une fonction $T(n)$
#### 1) _Simplification_ : 
On exprime $T(n)$ en appliquant les règles de simplifications des expressions grand-O.
_Exemple_ :
Si $T(n) = 2n^2 + 3$, on dit que l'algorithme est exécuté en un temps $O(n^2)$ ou temps quadratique

#### 2) _Proximité_ :
On choisit une fonction $g$ dont la croissance est le plus faible possible et pour laquelle $f \in O(g)$ 
_Exemple_ :
Si $T(n) = 2n^2 + 3$, alors 
1. $T(n) \in O(n^3)$, cependant n'exprime pas la croissance de $T(n)$
2. $T(n) \in O(2^n)$, cependant n'exprime pas la croissance de $T(n)$
3. $T(n) \in O(n^2)$, exprime la croissance de $T(n)$ car à proximité
