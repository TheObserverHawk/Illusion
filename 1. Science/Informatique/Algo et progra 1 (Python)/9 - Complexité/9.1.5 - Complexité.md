---
tags:
  - Note_done
  - Informatique
source: UMons - Algorithme et programmation 1
---

Link :
_Python : Complexité_
1. [[9.1.1 - Efficacité des algorithmes]]
1. [[9.1.2 - Temps CPU]]
2. [[9.1.3 - Fonction T(n), le temps d'exécution]]
3. [[9.1.4 - Grand-O]]
4. [[9.2.3 - Tri par fusion]]

# Définition
On parle de complexité pour évaluer l'efficacité théorique d'un algorithme, la croissance du temps mis par un algorithme en fonction de la taille de ses entrées.

### Comment évaluer la complexité des algorithmes ?
Nous allons appliquer la notation grand-O pour évaluer la complexité des algorithmes, en suivant quelques règles qui permettent de simplifier l’analyse et qui découlent directement des simplifications des expressions grand-O. 
Au lieu d’évaluer un temps d’exécution $T(n)$ précisément, les simplifications des expressions grand-O vont être appliquées directement, en fonction des structures utilisées dans le programme à analyser (boucles, instructions conditionnelles, etc.)

_Remarque_ :
1. Il existe beaucoup d’autres algorithmes de tris, joliment illustrés sur le site : https://www.toptal.com/developers/sorting-algorithms
2. La meilleure complexité possible pour un tri en en $O(n \log n)$ 
	1. (par ex., tri par fusion et quicksort).

## Règles de la complexité d'un algorithme 
#### 1) Instruction élémentaire
 Une instruction élémentaire est une instruction qui a un coût en temps $O(1)$ i.e. un temps constant car elle réalise une action en temps constant, quelle que soit la taille des entrées
 ##### _Règle_ :
 Par définition, une instruction élémentaire à un coût en temps $O(1)$
##### Exemple d'instructions élémentaires
1. Instruction d'affectation
2. Opérations arithmétiques sur des entiers ou des réels
3. Opérateur "return" 

#### 2) Somme et produit
On “additionne” la complexité des morceaux de code en ne gardant que la complexité la plus grande.

##### _Règle (sommation)_ :
Soit deux parties $A$ et $B$ d'un programme, exécutée de façon séquentielle.
Supposons que $T_A(n) = O(f_A(n))$ et $T_B(n) = O(f_B(n))$
Alors $$T_A(n)+T_B(n)=O(f_B(n)+f_A(n))=O(\max(f_B(n),f_A(n)))$$
##### _Règle (produit)_ :
Soit deux fonctions $f$ et $g$ 
La règle de produit dit $$O(f(n))\cdot O(g(n))=O(f(n)\cdot g(n)))$$
##### Exemple de somme et de produit
```python
def swap(a, b):
		temp = b
		b = a
		a = temp
		return (a,b)
```
Le temps de cet algorithme est en $O(1)$ car sa complexité est $$O(1) + O(1)+ O(1)+ O(1)$$ (sommation) ou $$4. O(1)= O(4) = O(1)$$ (produit) car les instructions 2, 3, 4 et 5 sont des instructions élémentaires et $O(4) = O(1)$ car règle de simplification du grand-O

#### 3) Appel de fonction
##### _Règle_ :
Le coût d’un appel à une fonction ou une méthode est équivalent au coût de l’exécution du corps de la fonction

##### Exemple d'appel de fonction
```python
def f(n):
		res = g(n)
		res += h(n)
		return res
```
Supposons que $g(n)$ soit en $O(n)$ et $h(n)$ soit en $O(n^2)$, alors le temps d'exécution de $f(n)$ est en $O(n) + O(n^2) + O(1) = O(n^2)$ 
#### 4) La boucle "for"
##### _Règle (for)_ :
Le temps d'exécution d'une boucle "for" est d'équivalent à :
$$\text{c. de la création d'une séquence + itérations (PDC) * c. du corps de la boucle}$$

_Remarque_ :
1. On peut sortir de la boucle à cause de "break", "return", d'où l'utilisation de PRC = Pire Des Cas
2. $n.O(1) = O(n.1) = O(n)$ car règle de complexité du produit
##### Exemple d'un algorithme avec la boucle "for"
```python
def sum_all(liste):
		res = 0
		for item in liste:
				res += item
		return res
```
On sait que "$\text{res} = 0$" est une instruction élémentaire, donc $O(1)$ 
Par la règle de la boucle "for", on a 
1. _Complexité de la création d'une séquence_ : Vu que la liste est déjà créée car elle est l'argument de la fonction, par conséquent, il n'y pas de coût lié à son utilisation donc $O(1)$ 
2. _Itérations_ : La boucle est exécuté $n$ fois
3. _Complexité du corps de la boucle_ : On sait que $\text{res } += \text{ item}$ est une instruction élémentaire, donc $O(1)$ 

Donc, le temps de cet algorithme est $$O(1) + n *O(1) = O(n)$$
#### 5) La boucle "while"
##### _Règle_ :
Le temps d’exécution d’une boucle "while" est équivalent à :
$$\text{itér (PDC) * (c. de l'évaluation de la cond + c. corps de la boucle) + c. pour sortir de la boucle i.e. cond}$$
_Remarque_ :
1. A chaque itération, la condition est évaluée
2. Pour sortir de la boucle, il faut évaluer la condition
##### Exemple
###### Exemple 1
```python
def sum_1_to_n(n):
		t = list(range(1, n+1))
		i = 0
		res = 0
		while i < n:
				res += t[i]
				i+= 1
		return res
```

On sait que l'instruction 2 est en $O(n)$ et les instructions 3, 4, 8 sont en $O(1)$ car instructions élémentaires

Par la règle de la boucle "while", on a 
1. _Itérations_ : La boucle est exécutée $n$ fois 
2. _Complexité d'évaluation de la condition_ : La complexité est en $O(1)$ car "$i < n$" est une instruction élémentaire
3. _Complexité du corps de la boucle_ : La complexité du corps est en $O(1)$ car les instructions 6 et 7 sont des instructions élémentaires
4. _Complexité de la condition pour sortir de la boucle_ : Pour sortir de la boucle, il faut "$i \ge n$", qui est une instruction élémentaire. Par conséquent la complexité est en $O(1)$ 

Donc la complexité de la boucle "while" est $$n.(O(1)+ O(1))+O(1) = O(n)$$
Par conséquent, la complexité de la fonction est $$O(n) + O(n) + 3.O(1) = O(n)$$
###### Exemple 2
```python
def is_include(t1, t2):
		i = 0
		n1 = len(t1)
		while i < len(t1) and t1[i] in t2:
				i += 1
		return i == n1
```

On sait que les instructions 2, 3 et 6 sont de $O(1)$ car ce sont des instructions élémentaires
Par la règle de la boucle "while", on a
1. _Itération_ : La boucle est exécuté $n_1$ fois car tant que $i < len(t_1)$ la boucle continue à s'exécuter
2. _Complexité d'évaluation de la boucle_ : On sait que l'évaluation de la boucle est "$i < len(t_1) \text{ and } t_1[i] \text{ in } t_2$", on sait que 
	1. "$i < \operatorname{len}(t_1)$" est en $O(1)$ car "$\operatorname{len}()$" est en $O(1)$, cependant 
	2. "$t_1[i] \text{ in } t_2$" est en $O(n_2)$ car "$in$" est en $O(n)$ puisque on recherche un élément dans la séquence avec l'opérateur "$in$"
	3. Par règle de sommation, on a que la complexité d'évaluation de la boucle est en $O(n_2)$ 
3. _Complexité du corps de la boucle_ : La complexité du corps est en $O(1)$ car l'instruction 5 est un instruction élémentaire
4. _Complexité de la condition de sortie de boucle_ : Pour sortir de la boucle, il faut cette condition "$i \ge len(t_1) \text{ or } t_1[i] \text{ not in } t_2$"
	1. Si on a $t_1[i] \text{ not in } t_2$, alors la complexité de la condition est en $O(n_2)$ 
	2. Sinon c'est $O(1)$ car $i \ge len(t_1)$

Donc, la complexité de la boucle "while" est $$O(n_1).(O(n_2)+O(1))+O(n_2) = O(n_1.n_2)$$ où $n_1,\ n_2$ sont les tailles de $t_1,\ t_2$ 
Donc le temps de cet algorithme, par la règle de sommation et du produit, est $$3.O(1) + O(n_1.n_2) = O(n_1.n_2)$$
#### 6) L'instruction conditionnelle
Dans le cas d’une instruction conditionnelle, on ne garde que le plus grand temps d’exécution de toutes les branches
##### _Règle (Instruction conditionnelle)_ :
Soit une instruction conditionnelle avec $k$ branches et $l$ conditions à évaluer 
Soit $T_i(n)$ le temps d'exécution de la $i$ème branches
1. Si $T_i(n) \in O(f_i(n))$ pour $i = 1,\ ...\ ,\ k$ et
si l'évaluation de la condition $j$ est en $O(g_j(n))$, pour $j = 1,\ ...\ ,\ l$, 
2. alors $$T(n)\in O\left(\max_i(f_i(n))+\max_j(g_j(n))\right)$$ où $T(n)$ est le temps d’exécution de l’instruction conditionnelle complète.

##### Exemple
```python
if x in t:
		print('x est dans t')
else:
		print('x pas dans t')
```
_Complexité de l'exécution des k branches_ : 
On a 2 branches de conditions, donc $i = 2$ car il y a le "`if`" dans l'instruction 1 et le "`else`" dans l'instruction 3, donc on a $2.O(1) = O(1)$ par la règle de la simplification du grand-O. 

_Complexité de l'évaluation de la condition_ : 
L’opération `x in t` est la condition à évaluer dans votre code. Cette opération a une complexité de $O(n)$ dans le pire des cas i.e. `t` est une liste, où le programme Python doit parcourir chaque élément de la liste pour vérifier si `x` est présent.

Dans ce cas, on a $\max_i (f_i(n)) = 1$ et $\max_i (g_i(n)) = n$ 
Par conséquent, le temps d'exécution de cette algorithme est $$T(n)\in O\left(\max_i(f_i(n))+\max_j(g_j(n))\right) = O(n+1) = O(n)$$

#### 7) Algorithmes récursifs
Pour analyser le temps d’exécution $T(n)$ d’une fonction récursive $f$ : 
1. on détermine une formule inductive pour $T(n)$ : 
	1. _Cas de base_ : Que vaut $T(n)$ quand il n’y a pas d’appel récursif ? 
	2. _Cas Gén._ : Que vaut $T(n)$ quand il y a des appels récursifs ? On estime le temps $T(n)$ par rapport au temps $T(k)$ des appels récursifs où $k$ est la taille appropriée des arguments (par exemple, $T(n−1))$
2. on évalue deux fois le temps du corps de $f$ comme précédemment, mais en gardant les termes $T(k)$ sous la forme d’inconnues. 
	1. quand on tombe dans le(s) cas de base 
	2. dans le cas général
3. Dans les expressions obtenues, on remplace les termes grand-O comme $O(h(n))$ par $c · h(n)$ où $c$ est une constante particulière.
4. On en déduit une forme non-inductive de $T(n)$

##### Exemple
```python
def sum_rec(n): 
		if n == 1: 
				return 1 
		else: 
				return sum_rec(n-1) + n
```
On a 
1. _Cas de base_ : 
Prenons $n =1$ 
Dans ce cas, la complexité de $T(1)$ est en $O(1)$ car seules les instructions 2 et 3 sont exécutées, de plus ce sont des instructions élémentaires
2. _Cas général_ :
Si $n > 1$, seules les instructions 2, 4 et 5 sont exécutées
Les instructions 2 et 4 sont en $O(1)$ car instruction élémentaire et le temps d'exécution de l'instruction 5 est $T(n-1)+O(1)$ 

Par conséquent, la forme inductive de $T(n)$ est donc $$\begin{array}{rcl} T(1)&=&O(1),\\T(n)&=&T(n-1)+O(1),\quad\forall n>1\end{array}$$

on remplace les expressions $O(h(n))$ par $c · h(n)$ càd une constante pour chaque expression : $$\begin{array}{rcl}T(1)&=&a,\\T(n)&=&T(n-1)+b,\quad\forall n>1\end{array}$$
on exprime $T(n)$ en fonction de $n$ et des constantes. 
Pour les premières valeurs on a : $$\begin{array}{rcl}T(1)&=&a,\\T(2)&=&T(1)+b=a+b,\\T(3)&=&T(2)+b=a+2b,\\T(4)&=&T(3)+b=a+3b.\end{array}$$
on déduit que $T(n) = a+ (n−1).b,\ ∀n ≥ 1$ (forme non-inductive)
on conclut que $T(n) ∈ O(n)$ puisque $a$ et $b$ sont des constantes
